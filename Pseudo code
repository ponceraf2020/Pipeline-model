# Import data
import pandas as pd
df = pd.read_excel("GSE24427 Experiment.xlsx")

# Define inputs and output
from sklearn.preprocessing import LabelEncoder
X = df.iloc[:, :-1].values
y = df.iloc[:, -1].values
le = LabelEncoder()
y = le.fit_transform(y)
le.classes_
le.transform(['high', 'low', 'medium'])

# Split data into training and testing
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = \
    train_test_split(X, y, 
                     test_size=0.20,
                     stratify=y,
                     random_state=1)

# Standardize data
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_std = scaler.fit_transform(X_train)
X_test_std = scaler.transform(X_test)

# Build the covariance matrix
import numpy as np
cov_mat = np.cov(X_train_std.T)
eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)
print('\nEigenvalues \n%s' % eigen_vals)

# Compute the explained variance ratio
tot = sum(eigen_vals)
var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]
cum_var_exp = np.cumsum(var_exp)

import matplotlib.pyplot as plt
plt.bar(range(0, 75), var_exp, alpha=0.1, align='center',
        label='Individual explained variance')
plt.step(range(0, 75), cum_var_exp, where='mid',
         label='Cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal component index')
plt.legend(loc='center right')
plt.tight_layout()
plt.show()
                     
# Define teh pipeline prediction model
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.neural_network import MLPClassifier
from sklearn.pipeline import make_pipeline

n_components = range(1, 20)
scores_pipe = []
for n in n_components:
    pipe_mlp = make_pipeline(StandardScaler(),
                        PCA(n_components=n),
                        MLPClassifier(
                                    solver="sgd",
                                    activation='tanh',
                                    random_state=1,
                                    learning_rate_init=0.01))
    pipe_mlp.fit(X_train, y_train)
    y_pred = pipe_mlp.predict(X_test)
    scores_pipe.append(pipe_mlp.score(X_test, y_test))

# Plot the optimal n_components for PCA
plt.plot(n_components, scores_pipe, 'r--')
plt.xlabel('Value of n_components for PCA')
plt.ylabel('Testing Accuracy')

# Evaluate the model for n_components=13
pipe_mlp = make_pipeline(StandardScaler(),
                        PCA(n_components=13),
                        MLPClassifier(
                                    solver="sgd",
                                    activation='tanh',
                                    random_state=1,
                                    learning_rate_init=0.01))
pipe_mlp.fit(X_train, y_train)
y_pred = pipe_mlp.predict(X_test)
print('Test Accuracy: %.3f' % pipe_mlp.score(X_test, y_test))

# Performance evaluation for 8 folds
import numpy as np
from sklearn.model_selection import StratifiedKFold
kfold = StratifiedKFold(n_splits=8).split(X_train, y_train)
scores_cv = []
for k, (train, test) in enumerate(kfold):
    pipe_mlp.fit(X_train[train], y_train[train])
    score = pipe_mlp.score(X_train[test], y_train[test])
    scores_cv.append(score)
    print('Fold: %2d, Class dist.: %s, Acc: %.3f' % (k+1,
          np.bincount(y_train[train]), score))
print('\nCV accuracy: %.3f +/- %.3f' % (np.mean(scores_cv), np.std(scores_cv)))

# Compute confusion matrix
from sklearn.metrics import confusion_matrix
pipe_mlp.fit(X_train, y_train)
y_pred = pipe_mlp.predict(X_test)
confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)

fig, ax = plt.subplots()
ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)
for i in range(confmat.shape[0]):
    for j in range(confmat.shape[1]):
        ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')

plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.tight_layout()
plt.show()
